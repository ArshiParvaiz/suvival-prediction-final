__author__ = 'marvinler'

import argparse
import datetime
import os
import pprint
import time
from time import gmtime, strftime
import math
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data
from torch.utils.tensorboard import SummaryWriter

from cod import N_PROCESSES, get_logger
from cod.data_processing.case_factory import split_svs_samples_casewise
from cod.data_processing.pytorch_dataset import Dataset
from cod.data_processing.MyBatchSampler import MyBatchSampler
from cod.models.image_classifiers import instantiate_model
from cod.models.mil_wrapper import MaxMinMIL
from cod.data_processing.main import main as end_to_end_data_preprocessing
from cod.data_processing.main import get_parser as pre_processing_get_parser
import matplotlib.pyplot as plt
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")

def define_args():
    # Get argparser from pre-processing module
    pre_processing_parser = pre_processing_get_parser()
    pre_processing_parser._action_groups[1].title = 'pre-processing options'
    to_remove_arg = pre_processing_parser._actions[1]
    to_remove_arg.container._remove_action(to_remove_arg)

    parser = argparse.ArgumentParser(description='Module to perform training of the segmentation method presented in '
                                                 'the paper. When no pre-processed data is available, perform '
                                                 'pre-processing using the pre-processing module.',
                                     parents=[pre_processing_parser], add_help=False,
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    # Arguments about the tile dataset
    parser.add_argument('--input-data-folder', type=str, default='./data', metavar='PATH',
                        help='path of parent folder containing preprocessed slides data; if data has not been '
                             'pre-processed yet, perform pre-processing first')
    # Parameters of the underlying model
    parser.add_argument('--underlying-model-type', type=str, default='efficientnetb0', metavar='MODEL',
                        help='type of underlying model to use: this is the instance classifier architecture')

    parser.add_argument('--pretrained', action='store_true', default=True,
                        help='use pretrained underlying architecture as init point')

    parser.add_argument('--load-from', type=str, default=None, metavar='PT_PATH',
                        help='model pt path from which to initialize training')
    parser.add_argument('--no-save-model', action='store_true', default=False, help='toggle model saving')
    parser.add_argument('--save-model-timesteps', type=int, default=5, help='number of epochs for each model save')
    parser.add_argument('--save-model-folder', type=str, default='./saved_models/',
                        help='folder in which to save model')

    # Training parameters
    parser.add_argument('--lr', type=float, default=0.0005, metavar='LR', help='learning rate')
    parser.add_argument('--reg', type=float, default=0.0001, metavar='R', help='weight decay')
    parser.add_argument('--epochs', type=int, default=20, metavar='N', help='number of epochs to train')
    parser.add_argument('--timestep-epoch', type=int, default=None, metavar='N', help='number of step per epoch')
    parser.add_argument('--no-cuda', action='store_true', default=False, help='disables CUDA training')
    parser.add_argument('--without-data-augmentation', action='store_true', default=False, help='use data augmentation')
    parser.add_argument('--patience', type=int, default=20, metavar='N_EPOCHS',
                        help='number of epochs (patience) for early stopping callback')
    parser.add_argument('--no-tensorboard', action='store_true', default=False,
                        help='disables tensorboard losses logging')

    # Dataset parameters
    parser.add_argument('--dataset-max-size', type=int, default=None, metavar='SEED',
                        help='max number of slides per split set train/val/test')
    parser.add_argument('--max-bag-size', type=int, default=21, metavar='SEED',
                        help='max number of instances per bag (will randomly select if there are more)')
    parser.add_argument('--val-size', type=float, default=0.20, metavar='PROPORTION',
                        help='%% of cases used for validation set')
    parser.add_argument('--test-size', type=float, default=0.20, metavar='PROPORTION',
                        help='%% of cases used for test set')

    parser.add_argument('--verbose', '-v', action='store_true', default=False,
                        help='put logger console handler level to logging.DEBUG')
    parser.add_argument('--seed', type=int, default=123, metavar='SEED', help='seed for datasets creation')

    args = parser.parse_args()

    hyper_parameters = {
        'input_data_folder': 'D:\\GBM\\output_folder',
        'gdc_executable': r'D:\Arshi\DOWNLOADS\gdc\gdc-client.exe',
        'manifest_file': 'D:\\GBM\\manifest.txt',
        'do_download': True,
        'source_slides_folder': 'D:\\GBM',

        # methods and underlying models
        'underlying_model_type': args.underlying_model_type.lower(),
        'underlying_model_pretrained': args.pretrained,
        'underlying_model_load_from': args.load_from,
        'save_model': not args.no_save_model,
        'save_model_timesteps': args.save_model_timesteps,

        # training control parameters
        'n_epochs': args.epochs,
        'n_timesteps_per_epoch': args.timestep_epoch,
        'learning_rate': args.lr,
        'weight_decay': args.reg,
        'early_stopping_patience': args.patience,
        # 'cuda': not args.no_cuda and torch.cuda.is_available(),
        'cuda':False,
        'models_save_folder': args.save_model_folder,

        # dataset control parameters
        'max_bag_size': args.max_bag_size,
        'dataset_max_size': args.dataset_max_size,
        'with_data_augmentation': not args.without_data_augmentation,
        'with_tensorboard': not args.no_tensorboard,
        'seed': args.seed,
        'val_size': args.val_size,
        'test_size': args.test_size,

        'verbose': args.verbose,
    }

    return hyper_parameters


def to_dataloader(dataset, for_training):
    assert isinstance(dataset, Dataset) or isinstance(dataset, torch.utils.data.Subset)
    batch_sampler = MyBatchSampler(dataset, batch_size=12, shuffle=True)
    return torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler, num_workers=0)
    # return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=for_training, num_workers=0)


def build_datasets(source_slides_folders, source_case_folders, model_input_width, hyper_parameters, logger):
    normalization_channels_mean = (0.6387467, 0.51136744, 0.6061169)
    normalization_channels_std = (0.31200314, 0.3260718, 0.30386254)

    # First load all data into a single Dataset
    whole_dataset = Dataset(slides_folders=source_slides_folders, cases_folders=source_case_folders, model_input_size=model_input_width,
                            is_training=False, max_bag_size=hyper_parameters['max_bag_size'],
                            logger=logger, max_dataset_size=hyper_parameters['dataset_max_size'],
                            with_data_augmentation=hyper_parameters['with_data_augmentation'],
                            seed=hyper_parameters['seed'],
                            normalization_mean=normalization_channels_mean,
                            normalization_std=normalization_channels_std)
    whole_cases_ids = whole_dataset.cases_ids
    whole_indexes = list(range(len(whole_dataset)))

    val_size = hyper_parameters['val_size']
    test_size = hyper_parameters['test_size']
    train_idx, val_idx, test_idx = split_svs_samples_casewise(whole_indexes, whole_cases_ids,
                                                              val_size=val_size, test_size=test_size,
                                                              seed=hyper_parameters['seed'])

    train_dataset = torch.utils.data.Subset(whole_dataset, train_idx)
    val_dataset = torch.utils.data.Subset(whole_dataset, val_idx)
    test_dataset = torch.utils.data.Subset(whole_dataset, test_idx)
    train_dataset.dataset.is_training = True
    # train_dataset.dataset.transform = train_dataset.dataset._define_data_transforms(normalization_channels_mean,
    #                                                                                 normalization_channels_std)

    return train_dataset, val_dataset, test_dataset


def early_stopping(val_losses, patience):
    """ Return (True, min achieved val loss) if no val losses is under the minimal achieved val loss for patience
        epochs, otherwise (False, None) """
    # Do not stop until enough epochs have been made
    if len(val_losses) < patience:
        return False, None

    best_val_loss = np.min(val_losses)
    if not np.any(val_losses[-patience:] <= best_val_loss):
        return True, best_val_loss
    return False, None


def perform_epoch(model, optimizer,scheduler, epoch, dataloader, hyper_parameters, is_training, logger, set_name, dataset_length, summary_writer=None):
    if is_training:
        model.train()
    else:
        model.eval()

    epoch_loss = []

    epoch_cases = torch.Tensor()
    epoch_cases = epoch_cases.to(device)

    epoch_predictions = torch.Tensor()
    epoch_predictions = epoch_predictions.to(device)

    epoch_cases_times = torch.Tensor()
    epoch_cases_times = epoch_cases_times.to(device)
    epoch_cases_ids = []

    for batch_idx, (cases_instances, cases_labels, cases_times, cases_clinical, cases_id) in enumerate(dataloader):
        # reset gradients
        if is_training:
            optimizer.zero_grad()

        cases_id = list(cases_id)
        epoch_cases_ids.append(cases_id)

        cases_instances = cases_instances.to(device)
        cases_labels = cases_labels.to(device)
        list_of_floats = [float(x) for x in cases_times]
        cases_times = torch.tensor(list_of_floats, dtype=torch.float64)
        cases_times = cases_times.to(device)



        # Forward pass
        instances_predictions = model(cases_instances)

        epoch_cases = torch.cat((epoch_cases,cases_labels), dim=0)
        epoch_predictions = torch.cat((epoch_predictions,instances_predictions), dim=0)
        epoch_cases_times = torch.cat((epoch_cases_times,cases_times), dim=0)





        loss = model.loss(instances_predictions, cases_labels,cases_times)

        epoch_loss.append(loss.item())
        if is_training:
            loss.backward()
            optimizer.step()
            # scheduler.step()
            # print(optimizer.param_groups[0]["lr"])

    _, _, _, _, _, numerator, denominator = model.concordance_index_censored(epoch_cases, epoch_cases_times, epoch_predictions, tied_tol=1e-8)

    epoch_cindex = numerator / denominator
    mean_epoch_loss = np.mean(epoch_loss)
    std_epoch_loss = np.std(epoch_loss)
    if epoch_cindex > 0.75:
        print(epoch_cases_ids)
        print(epoch_predictions)

    if hyper_parameters['with_tensorboard']:
        summary_writer.add_scalar('loss_mean/' + set_name, mean_epoch_loss, epoch)
        summary_writer.add_scalar('loss_std/' + set_name, std_epoch_loss, epoch)

    if is_training:
        log_msg = ' ' * 100 + set_name + '=> Epoch: {:2d}/{}  loss={:.4f}+/-{:.4f} C-index = {:.4f}'.format(epoch+1, hyper_parameters['n_epochs'],
                                                                 mean_epoch_loss, std_epoch_loss,epoch_cindex)
    else:
        log_msg = ' ' * 100 + set_name + '  loss={:.4f}+/-{:.4f} C-index = {:.4f}'.format(mean_epoch_loss, std_epoch_loss, epoch_cindex)

    logger.info(log_msg)

    return mean_epoch_loss, epoch_cindex


def main(hyper_parameters):

    logger = get_logger(filename_handler='code.' + os.path.basename(__file__) +  '.log',
                        verbose=hyper_parameters['verbose'])
    logger.info('Hyper parameters')
    logger.info(pprint.pformat(hyper_parameters, indent=4))

    # Pre-processing should have been done beforehand, retrieve data by specifying data preprocessing output folder
    slides_folders, cases_folders= end_to_end_data_preprocessing(hyper_parameters['input_data_folder'],
                                                    hyper_parameters['do_download'], hyper_parameters['gdc_executable'],
                                                   hyper_parameters['manifest_file'],
                                                   hyper_parameters['source_slides_folder'])

    logger.info('Initializing model... ')
    if not os.path.exists(hyper_parameters['models_save_folder']):
        os.makedirs(hyper_parameters['models_save_folder'])
    # Instantiate instance classifier model, then MIL wrapper
    instance_classifier, input_width = instantiate_model(model_type=hyper_parameters['underlying_model_type'],
                                                         pretrained=hyper_parameters['underlying_model_pretrained'],
                                                         )
    logger.info(f'  {hyper_parameters["underlying_model_type"]} architecture initialized')

    mil_model = MaxMinMIL(instance_classifier)
    mil_model = mil_model.to(device)

    PATH = 'D:\Arshi\DESKTOP\Survival-Prediction\cod\saved_models\model1.pth'
    mil_model.load_state_dict(torch.load(PATH, map_location='cpu'))

    logger.info('mil wrapper initialized')



    optimizer = optim.Adam(mil_model.parameters(), lr=hyper_parameters['learning_rate'],
                           weight_decay=hyper_parameters['weight_decay'])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=hyper_parameters['n_epochs'], eta_min=0,
                                                           last_epoch=-1)

    # Load data and split case-wise into train, val and test sets
    logger.info('Pre-loading all data...')
    cases_folders = list(set(cases_folders))
    train_dataset, val_dataset, test_dataset = build_datasets(source_slides_folders=slides_folders,
                                                              source_case_folders=cases_folders,
                                                              model_input_width=input_width,
                                                              hyper_parameters=hyper_parameters,
                                                              logger=logger)
    logger.info('Train size %d' % len(train_dataset))
    logger.info('Val size %d' % len(val_dataset))
    logger.info('Test size %d' % len(test_dataset))

    train_dataloader = to_dataloader(train_dataset, True)
    val_dataloader = to_dataloader(val_dataset, False) if len(val_dataset) else None
    test_dataloader = to_dataloader(test_dataset, False) if len(test_dataset) else None

    # Instantiate summary writer if tensorboard activated
    if hyper_parameters['with_tensorboard']:
        summary_writer_filename = 'summary_'
        summary_writer_folder_path = os.path.join('tensorboard', summary_writer_filename)
        summary_writer = SummaryWriter(log_dir=summary_writer_folder_path)
    else:
        summary_writer = None

    val_losses = []
    val_cindexes = []

    # Perform training loop: for each epoch, train and validate
    logger.info('Starting training...')
    start_training_time = time.time()
    epoch_train_loss = []
    epoch_train_cindex = []

    for epoch in range(hyper_parameters['n_epochs']):
        # Train
        train_loss, train_cindex = perform_epoch(mil_model, optimizer, scheduler, epoch, train_dataloader,
                                                   hyper_parameters=hyper_parameters, is_training=True,
                                                   logger=logger, set_name='training', dataset_length = len(train_dataset), summary_writer=summary_writer)

        epoch_train_loss.append(train_loss)
        epoch_train_cindex.append(train_cindex)

        # Validate
        if val_dataloader:
            with torch.no_grad():
                val_loss, val_cindex = perform_epoch(mil_model, optimizer, scheduler, epoch, val_dataloader,
                                            hyper_parameters=hyper_parameters, is_training=False,
                                            logger=logger, set_name='validation', dataset_length = len(val_dataset), summary_writer=summary_writer)
                val_losses.append(val_loss)
                val_cindexes.append(val_cindex)
            if (epoch+1) % 10 == 0:
                e = str(epoch+1)
                PATH = 'D:\\Arshi\\DESKTOP\\Survival-Prediction\\cod\\saved_models\\model' + e + '.pth'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': mil_model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': train_loss,
                }, PATH)


    plt.plot(epoch_train_loss, label='train_loss')
    plt.plot(val_losses, label='val_loss')
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.show()

    # Create the second plot for C-index
    plt.plot(epoch_train_cindex, label='train c-index')
    plt.plot(val_cindexes, label='val c-index')
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('C-index')
    plt.show()


    log_msg = 'Total Train loss={:.4f}'.format(np.mean(epoch_train_loss))
    logger.info(log_msg)

    log_msg = 'Total Train C-index={:.4f}'.format(np.mean(epoch_train_cindex))
    logger.info(log_msg)
    print('--------------------------------------------------------------------------')

    log_msg = 'Total Validation loss={:.4f}'.format(np.mean(val_losses))
    logger.info(log_msg)

    log_msg = 'Total Validation C-index={:.4f}'.format(np.mean(val_cindexes))
    logger.info(log_msg)

    logger.warning('Total training time %s' % (time.time() - start_training_time))

    # Test
    if test_dataloader:
        logger.info('Starting testing...')
        with torch.no_grad():
            test_loss, test_cindex = perform_epoch(mil_model, optimizer,scheduler, -1, test_dataloader, hyper_parameters=hyper_parameters, dataset_length = len(val_dataset),
                          is_training=False, logger=logger, set_name='test', summary_writer=summary_writer)

    print('--------------------------------------------------------------------------')

    log_msg = 'Test loss={:.4f}'.format(test_loss )
    logger.info(log_msg)

    log_msg = 'Test C-index={:.4f}'.format(test_cindex)
    logger.info(log_msg)
    PATH = 'D:\\Arshi\\DESKTOP\\Survival-Prediction\\cod\\saved_models\\model2.pth'
    torch.save(mil_model.state_dict(), PATH)
    return


if __name__ == '__main__':
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
    main(define_args())
